{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c1fcef",
   "metadata": {},
   "source": [
    "---\n",
    "# Agente de IA da Alura\n",
    "## O Problema\n",
    "> A empresa de educação Alura deseja criar um chatbot inteligente para auxiliar futuros alunos. Este assistente deve responder a perguntas específicas sobre o conteúdo do curso de RAG, utilizando os próprios PDFs das aulas como sua base de conhecimento. O objetivo é fornecer respostas precisas e confiáveis, baseadas exclusivamente no material didático.\n",
    "\n",
    "## A Solução\n",
    "> Construir um sistema RAG completo, seguindo os passos e utilizando as ferramentas discutidas nas aulas\n",
    "\n",
    "### Etapas\n",
    "- **Configuração do ambiente**: Instalação das bibliotecas e configuração do modelo local\n",
    "- **Pipeline de ingestão de dados (ETL)**\n",
    "  - *Extração*: Carregar os docs PDF das aulas\n",
    "  - *Transformação*: Aplicar estratégias de chunking adaptativo e gerar embeddings\n",
    "  - *Carregamento*: Indexar os chunks e seus embeddings em um banco de dados vetorial, o Chroma\n",
    "- **Construção de um Sistema de Recuperação Avançado**: Implementar busca híbrida para combinar busca lexical *(BM25)* e semântica\n",
    "- **Criação de uma Cadeia de Conversação Robsuta**\n",
    "  - Implementar a *ConversationalRetrievalChain*\n",
    "  - Integrar gerenciamento de memória para manter o contexto do diálogo\n",
    "  - Aplicar transformação de consulta *(Query Transformation)*, que ocorre internamente na cadeia para refinar as perguntas com base no histórico\n",
    "- **Avaliação do Sistema com RAGAS**: Avaliar a performance do sistema utilizando as métricas específicas do RAGAS, como *Faithfulness*, *Answer Relevancy*, *Context Precision* e *Context Recall*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependências\n",
    "\n",
    "# pip install langchain-classic langchain-core langchain-community \n",
    "\n",
    "# pip install langchain-text-splitters transformers\n",
    "\n",
    "# pip install langchain-openai langchain-huggingface\n",
    "\n",
    "# pip install chromadb pypdf rank_bm25\n",
    "\n",
    "# pip install ragas datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b8d01",
   "metadata": {},
   "source": [
    "### Pipeline de Ingestão de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24f1fa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"documentos_curso/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb87ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks criados: 98\n"
     ]
    }
   ],
   "source": [
    "# Transform\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Define que agora o tamanho dos chunks serão em TOKENS, não em CARACTÉRES\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
    "    length_function=count_tokens\n",
    ")\n",
    "\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total de chunks criados: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba334f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 61 tokens\n",
      "Chunk 2: 11 tokens\n",
      "Chunk 3: 50 tokens\n",
      "Chunk 4: 41 tokens\n",
      "Chunk 5: 87 tokens\n",
      "Chunk 6: 91 tokens\n",
      "Chunk 7: 61 tokens\n",
      "Chunk 8: 11 tokens\n",
      "Chunk 9: 35 tokens\n",
      "Chunk 10: 64 tokens\n",
      "Chunk 11: 82 tokens\n",
      "Chunk 12: 93 tokens\n",
      "Chunk 13: 84 tokens\n",
      "Chunk 14: 127 tokens\n",
      "Chunk 15: 109 tokens\n",
      "Chunk 16: 130 tokens\n",
      "Chunk 17: 184 tokens\n",
      "Chunk 18: 126 tokens\n",
      "Chunk 19: 95 tokens\n",
      "Chunk 20: 145 tokens\n",
      "Chunk 21: 201 tokens\n",
      "Chunk 22: 66 tokens\n",
      "Chunk 23: 11 tokens\n",
      "Chunk 24: 9 tokens\n",
      "Chunk 25: 89 tokens\n",
      "Chunk 26: 162 tokens\n",
      "Chunk 27: 74 tokens\n",
      "Chunk 28: 73 tokens\n",
      "Chunk 29: 67 tokens\n",
      "Chunk 30: 133 tokens\n",
      "Chunk 31: 142 tokens\n",
      "Chunk 32: 252 tokens\n",
      "Chunk 33: 125 tokens\n",
      "Chunk 34: 119 tokens\n",
      "Chunk 35: 108 tokens\n",
      "Chunk 36: 333 tokens\n",
      "Chunk 37: 61 tokens\n",
      "Chunk 38: 11 tokens\n",
      "Chunk 39: 11 tokens\n",
      "Chunk 40: 78 tokens\n",
      "Chunk 41: 76 tokens\n",
      "Chunk 42: 33 tokens\n",
      "Chunk 43: 70 tokens\n",
      "Chunk 44: 77 tokens\n",
      "Chunk 45: 71 tokens\n",
      "Chunk 46: 62 tokens\n",
      "Chunk 47: 68 tokens\n",
      "Chunk 48: 65 tokens\n",
      "Chunk 49: 62 tokens\n",
      "Chunk 50: 11 tokens\n",
      "Chunk 51: 12 tokens\n",
      "Chunk 52: 85 tokens\n",
      "Chunk 53: 86 tokens\n",
      "Chunk 54: 56 tokens\n",
      "Chunk 55: 52 tokens\n",
      "Chunk 56: 48 tokens\n",
      "Chunk 57: 45 tokens\n",
      "Chunk 58: 58 tokens\n",
      "Chunk 59: 49 tokens\n",
      "Chunk 60: 43 tokens\n",
      "Chunk 61: 90 tokens\n",
      "Chunk 62: 40 tokens\n",
      "Chunk 63: 116 tokens\n",
      "Chunk 64: 62 tokens\n",
      "Chunk 65: 11 tokens\n",
      "Chunk 66: 13 tokens\n",
      "Chunk 67: 58 tokens\n",
      "Chunk 68: 111 tokens\n",
      "Chunk 69: 48 tokens\n",
      "Chunk 70: 135 tokens\n",
      "Chunk 71: 48 tokens\n",
      "Chunk 72: 129 tokens\n",
      "Chunk 73: 55 tokens\n",
      "Chunk 74: 115 tokens\n",
      "Chunk 75: 66 tokens\n",
      "Chunk 76: 11 tokens\n",
      "Chunk 77: 16 tokens\n",
      "Chunk 78: 40 tokens\n",
      "Chunk 79: 107 tokens\n",
      "Chunk 80: 57 tokens\n",
      "Chunk 81: 104 tokens\n",
      "Chunk 82: 49 tokens\n",
      "Chunk 83: 110 tokens\n",
      "Chunk 84: 43 tokens\n",
      "Chunk 85: 160 tokens\n",
      "Chunk 86: 65 tokens\n",
      "Chunk 87: 181 tokens\n",
      "Chunk 88: 62 tokens\n",
      "Chunk 89: 11 tokens\n",
      "Chunk 90: 12 tokens\n",
      "Chunk 91: 54 tokens\n",
      "Chunk 92: 107 tokens\n",
      "Chunk 93: 36 tokens\n",
      "Chunk 94: 52 tokens\n",
      "Chunk 95: 40 tokens\n",
      "Chunk 96: 61 tokens\n",
      "Chunk 97: 46 tokens\n",
      "Chunk 98: 201 tokens\n"
     ]
    }
   ],
   "source": [
    "# Verificar quantidade de TOKENS por chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    token_count = count_tokens(chunk.page_content)\n",
    "    print(f\"Chunk {i+1}: {token_count} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1921d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4364463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banco de dados criado com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents = chunks,\n",
    "    embedding = embeddings_model\n",
    ")\n",
    "\n",
    "print(\"Banco de dados criado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2dc307",
   "metadata": {},
   "source": [
    "### Sistema de Recuperação Avançado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28321cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recuperador de Busca Híbrida Configurado\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# 1. Recuperador Lexical (BM25)\n",
    "bm25_retriever = BM25Retriever.from_documents(chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# 2. Recuperador Vetorial (a partir do ChromaDB)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 3. EnsembleRetriever: combina os resultados\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.4, 0.6] \n",
    ")\n",
    "\n",
    "print(\"Recuperador de Busca Híbrida Configurado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca4cdd",
   "metadata": {},
   "source": [
    "### Cadeia de Conversação Robusta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a43ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_16480\\1398636626.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "# Modelo local\n",
    "llm = ChatOpenAI(\n",
    "    model = \"google/gemma-2-9b\",\n",
    "    openai_api_base=\"http://127.0.0.1:1234/v1\",\n",
    "    openai_api_key=\"lm-studio\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key = \"chat_history\",\n",
    "    output_key = \"answer\",\n",
    "    return_messages = True\n",
    ")\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm = llm,\n",
    "    retriever = ensemble_retriever,\n",
    "    memory = memory,\n",
    "    verbose = False,\n",
    "    return_source_documents = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b4880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta_teste1 = qa_chain.invoke({\"question\": \"O que é chunking adaptativo?\"})\n",
    "print(resposta_teste1[\"answer\"])\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb323a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta_teste2 = qa_chain.invoke({\"question\": \"E quais as principais estratégias?\"})\n",
    "print(resposta_teste2[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8176c0c2",
   "metadata": {},
   "source": [
    "### Avaliação do Sistema com RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67917ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_16480\\4248735931.py:3: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import (\n",
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_16480\\4248735931.py:3: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import (\n",
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_16480\\4248735931.py:3: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import (\n",
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_16480\\4248735931.py:3: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import (\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "\n",
    "# 1. Conjunto de dados para avaliação no RAGAS\n",
    "\n",
    "perguntas = [\n",
    "    \"O que é RAG e qual problema ele soluciona?\",\n",
    "    \"Quais os componentes essenciais do RAG?\",\n",
    "    \"Qual a diferença entre busca lexical e semântica?\",\n",
    "    \"O que mede a métrica faithfulness do RAGAS?\"\n",
    "]\n",
    "\n",
    "respostas_puro = [\n",
    "    \"RAG (Retrieval-Augmented Generation) é uma arquitetura que combina um motor de busca para recuperar informações com um L\",\n",
    "    \"Os componentes essenciais são: Embeddings, Banco de Dados Vetorial, chunking e um Modelo de Linguagem (LLM).\",\n",
    "    \"Busca lexical (como BM25) encontra correspondências exatas de termos, enquanto a busca semântica captura o significado e \",\n",
    "    \"A métrica Faithfulness mede se a resposta gerada é suportada e factualmente consistente com os documentos recuperados, e\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f94b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Gere as respostas e contextos com a nossa cadeia\n",
    "respostas_geradas = []\n",
    "contextos_recuperados = []\n",
    "for question in perguntas:\n",
    "    result = qa_chain.invoke({\"question\": question})\n",
    "    respostas_geradas.append(result['answer'])\n",
    "    contextos_recuperados.append([doc.page_content for doc in result['source_documents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc252e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset gerado no formato esperado pelo RAGAS\n",
    "dataset_dict = {\n",
    "    'question': perguntas,\n",
    "    'answer': respostas_geradas,\n",
    "    'contexts': contextos_recuperados,\n",
    "    'ground_truth': respostas_puro\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33d5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/4 [00:00<?, ?it/s]LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "Evaluating:  50%|█████     | 2/4 [01:44<01:43, 51.57s/it]Exception raised in Job[2]: TimeoutError()\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 4/4 [03:00<00:00, 45.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados da Avaliação com RAGAS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O que é RAG e qual problema ele soluciona?</td>\n",
       "      <td>[FLUXO COMPLETO DO RAG\\nO ciclo de vida comple...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) combina a...</td>\n",
       "      <td>RAG (Retrieval-Augmented Generation) é uma arq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.138772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   user_input  \\\n",
       "0  O que é RAG e qual problema ele soluciona?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [FLUXO COMPLETO DO RAG\\nO ciclo de vida comple...   \n",
       "\n",
       "                                            response  \\\n",
       "0  RAG (Retrieval-Augmented Generation) combina a...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  RAG (Retrieval-Augmented Generation) é uma arq...           NaN   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.138772                NaN             1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Execute a avaliação\n",
    "evaluation_result = evaluate(\n",
    "    dataset = dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    llm=llm,\n",
    "    embeddings=embeddings_model\n",
    ")\n",
    "\n",
    "# 5. Analise dos resultados\n",
    "df_resultados = evaluation_result.to_pandas()\n",
    "print(\"\\nResultados da Avaliação com RAGAS:\")\n",
    "display(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
